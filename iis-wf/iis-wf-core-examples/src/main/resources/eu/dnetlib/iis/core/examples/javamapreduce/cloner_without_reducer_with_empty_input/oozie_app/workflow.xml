<workflow-app xmlns="uri:oozie:workflow:0.4" name="test-core_examples_javamapreduce_cloner_without_reducer_with_empty_input">

    <global>
        <job-tracker>${jobTracker}</job-tracker>
        <name-node>${nameNode}</name-node>
        <configuration>
            <property>
                <name>mapreduce.job.queuename</name>
                <value>${queueName}</value>
            </property>
            <property>
                <name>oozie.launcher.mapred.job.queue.name</name>
                <value>${oozieLauncherQueueName}</value>
            </property>
        </configuration>
    </global>
    
    
	<start to="generate-schema" />

	<action name="generate-schema">
	    <java>
	        <main-class>eu.dnetlib.iis.common.javamapreduce.hack.AvroSchemaGenerator</main-class>
	        <arg>eu.dnetlib.iis.core.examples.schemas.documentandauthor.Person</arg>
	        <arg>org.apache.avro.Schema.Type.NULL</arg>
	        <capture-output />
	    </java>
	    <ok to="producer" />
	    <error to="fail" />
	</action>
	
	<action name="producer">
		<java>
			<!-- The data generated by this node is deleted in this section -->
			<prepare>
				<delete path="${nameNode}${workingDir}/producer" />
				<mkdir path="${nameNode}${workingDir}/producer" />
			</prepare>
			<!-- This is simple wrapper for the Java code -->
			<main-class>eu.dnetlib.iis.common.java.ProcessWrapper</main-class>
			<!-- The business Java code that gets to be executed -->
			<arg>eu.dnetlib.iis.common.java.jsonworkflownodes.Producer</arg>
			<!-- Specification of the output ports -->
			<arg>-C{person, 
				eu.dnetlib.iis.core.examples.schemas.documentandauthor.Person,
				eu/dnetlib/iis/core/examples/data/empty.json}</arg>
			<!-- All input and output ports have to be bound to paths in HDFS -->
			<arg>-Operson=${workingDir}/producer/person</arg>
		</java>
		<ok to="cloner" />
		<error to="fail" />
	</action>
	<action name="cloner">
		<map-reduce>
			<!-- The data generated by this node is deleted in this section -->
			<prepare>
				<delete path="${nameNode}${workingDir}/cloner" />
				<mkdir path="${nameNode}${workingDir}/cloner" />
			</prepare>
			<configuration>
				<!-- # This is a standard set of options that stays the same regardless 
					of a concrete definition of map-reduce job -->

				<!-- ## Various options -->

				<!--This property seems to not be needed -->
				<!--<property> <name>mapreduce.job.queuename</name> <value>${queueName}</value> 
					</property> -->
				<property>
					<name>mapreduce.job.inputformat.class</name>
					<value>org.apache.avro.mapreduce.AvroKeyInputFormat</value>
				</property>
				<property>
					<name>mapreduce.job.outputformat.class</name>
					<value>org.apache.avro.mapreduce.AvroKeyOutputFormat</value>
				</property>
				<property>
					<name>mapreduce.map.output.key.class</name>
					<value>org.apache.avro.mapred.AvroKey</value>
				</property>
				<!-- There is no value written in the map phase (because the
				output is passed as a key) -->
				<property>
					<name>mapreduce.map.output.value.class</name>
					<!-- <value>org.apache.avro.mapred.AvroValue</value>-->
					<value>org.apache.hadoop.io.NullWritable</value>
				</property>
				<!-- Since there is no reduce phase, there should be no
				reduce tasks -->
				 <property>
                    <name>mapreduce.job.reduces</name>
                    <value>0</value>
                </property>
				 
				 
				<property>
					<name>mapreduce.job.output.key.class</name>
					<value>org.apache.avro.mapred.AvroKey</value>
					<!-- <value>org.apache.hadoop.io.NullWritable</value>-->
				</property>
				<property>
					<name>mapreduce.job.output.value.class</name>
					<value>org.apache.avro.mapred.AvroValue</value>
					<!-- <value>org.apache.hadoop.io.NullWritable</value>-->
				</property>
				<property>
					<name>mapreduce.job.output.key.comparator.class</name>
					<value>org.apache.avro.hadoop.io.AvroKeyComparator</value>
				</property>
				<property>
					<name>io.serializations</name>
					<value>org.apache.hadoop.io.serializer.WritableSerialization,org.apache.hadoop.io.serializer.avro.AvroSpecificSerialization,org.apache.hadoop.io.serializer.avro.AvroReflectSerialization,org.apache.avro.hadoop.io.AvroSerialization
					</value>
				</property>
				<property>
					<name>mapreduce.job.output.group.comparator.class</name>
					<value>org.apache.avro.hadoop.io.AvroKeyComparator</value>
				</property>
				<property>
					<name>rpc.engine.org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolPB
					</name>
					<value>org.apache.hadoop.ipc.ProtobufRpcEngine</value>
				</property>

				<!-- ## This is required for new MapReduce API usage -->

				<property>
					<name>mapred.mapper.new-api</name>
					<value>true</value>
				</property>
				<property>
					<name>mapred.reducer.new-api</name>
					<value>true</value>
				</property>

				<!-- # Job-specific options -->

				<!-- ## Classes of mapper and reducer -->

				<property>
					<name>mapreduce.job.map.class</name>
					<value>eu.dnetlib.iis.core.examples.javamapreduce.PersonClonerWithoutReducerMapper</value>
				</property>
				
				<!-- No reducer -->

				<!-- ## Schemas -->

				<!-- ### Shema of the data ingested by the mapper. To be more precise, 
					it's the schema of Avro data passed as template parameter of the AvroKey 
					object passed to mapper -->
				<property>
					<name>avro.schema.input.key</name>
					<value>${wf:actionData('generate-schema')['eu.dnetlib.iis.core.examples.schemas.documentandauthor.Person']}</value>
				</property>

				<!-- ### Schemas of the data produced by the mapper -->

				<!-- #### Schema of the key produced by the mapper. 
				
				To be more precise,	it's the schema of Avro data produced 
				by the mapper and passed forward as	template parameter 
				of AvroKey object. -->
		        <property>
                    <name>avro.serialization.key.reader.schema</name>
                    <value>${wf:actionData('generate-schema')['eu.dnetlib.iis.core.examples.schemas.documentandauthor.Person']}</value>
                </property>
                <property>
                    <name>avro.serialization.key.writer.schema</name>
                    <value>${wf:actionData('generate-schema')['eu.dnetlib.iis.core.examples.schemas.documentandauthor.Person']}</value>
                </property>

				<!-- #### Schema of the value produced by the mapper. 
				
					To be more precise, 
					it's the schema of Avro data produced by the mapper and passed forward as 
					template parameter of AvroValue object. -->
					
				<!-- As a convention, we're setting "null" values 
				since mapper does not produce "values" that will be passed to 
				reducer in this example (probably any other valid Avro schema 
				would be OK as well).-->
		        <property>
                    <name>avro.serialization.value.reader.schema</name>
                    <value>${wf:actionData('generate-schema')['org.apache.avro.Schema.Type.NULL']}</value>
                </property>
                <property>
                    <name>avro.serialization.value.writer.schema</name>
                    <value>${wf:actionData('generate-schema')['org.apache.avro.Schema.Type.NULL']}</value>
                </property>

				<!-- ### Schema of the data produced by the job -->
		        <property>
                    <name>avro.schema.output.key</name>
                    <value>${wf:actionData('generate-schema')['eu.dnetlib.iis.core.examples.schemas.documentandauthor.Person']}</value>
                </property>

				<!-- ## Specification of the input and output data store -->

				<property>
					<name>mapreduce.input.fileinputformat.inputdir</name>
					<value>${workingDir}/producer/person</value>
				</property>
				<property>
					<name>mapreduce.output.fileoutputformat.outputdir</name>
					<value>${workingDir}/cloner/person</value>
				</property>
				
				<!-- ##  Workflow node parameters -->
				
				<property>
					<name>copiesCount</name>
					<value>2</value>
				</property>
			</configuration>
		</map-reduce>
		<ok to="consumer" />
		<error to="fail" />
	</action>
	<action name="consumer">
		<java>
			<!-- This is simple wrapper for the Java code -->
			<main-class>eu.dnetlib.iis.common.java.ProcessWrapper</main-class>
			<!-- The business Java code that gets to be executed -->
			<arg>eu.dnetlib.iis.common.java.jsonworkflownodes.TestingConsumer</arg>
			<!-- Specification of the input ports -->
			<arg>-C{person, 
				eu.dnetlib.iis.core.examples.schemas.documentandauthor.Person,
				eu/dnetlib/iis/core/examples/data/empty.json}</arg>
			<!-- All input and output ports have to be bound to paths in HDFS -->
			<arg>-Iperson=${workingDir}/cloner/person</arg>
		</java>
		<ok to="end" />
		<error to="fail" />
	</action>
	<kill name="fail">
		<message>Unfortunately, the process failed -- error message:
			[${wf:errorMessage(wf:lastErrorNode())}]
		</message>
	</kill>
	<end name="end" />
</workflow-app>
