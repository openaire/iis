<?xml version="1.0"?>
<workflow-app xmlns="uri:oozie:workflow:0.4" name="documentsclassification_main">
	
	<parameters>
		<property>
			<name>input_documents</name>
			<description>input source documents</description>
		</property>
		<property>
			<name>output_document_to_document_classes</name>
			<description>output document to document classes</description>
		</property>
        <property>
            <name>output_report_root_path</name>
            <description>base directory for storing reports</description>
        </property>
        <property>
            <name>output_report_relative_path</name>
            <value>document_classification</value>
            <description>directory for storing report (relative to output_report_root_path)</description>
        </property>
        <property>
            <name>sparkDriverMemory</name>
            <description>memory for driver process</description>
        </property>
        <property>
            <name>sparkExecutorMemory</name>
            <description>memory for individual executor</description>
        </property>
        <property>
            <name>sparkExecutorCores</name>
            <description>number of cores used by single executor</description>
        </property>
        <property>
            <name>oozieActionShareLibForSpark2</name>
            <description>oozie action sharelib for spark 2.*</description>
        </property>
        <property>
            <name>spark2ExtraListeners</name>
            <value>com.cloudera.spark.lineage.NavigatorAppListener</value>
            <description>spark 2.* extra listeners classname</description>
        </property>
        <property>
            <name>spark2SqlQueryExecutionListeners</name>
            <value>com.cloudera.spark.lineage.NavigatorQueryListener</value>
            <description>spark 2.* sql query execution listeners classname</description>
        </property>
        <property>
            <name>spark2YarnHistoryServerAddress</name>
            <description>spark 2.* yarn history server address</description>
        </property>
        <property>
            <name>spark2EventLogDir</name>
            <description>spark 2.* event log dir location</description>
        </property>
        <property>
            <name>numberOfPartitions</name>
            <value>$UNDEFINED$</value>
            <description>number of partitions the input data should be sliced into. 
            Affects the number of tasks executed on the input data and each task execution time (more tasks less time each executor takes). 
            Relying on default value when $UNDEFINED$.</description>
        </property>
    </parameters>

    <global>
        <job-tracker>${jobTracker}</job-tracker>
        <name-node>${nameNode}</name-node>
        <configuration>
            <property>
                <name>mapreduce.job.queuename</name>
                <value>${queueName}</value>
            </property>
            <property>
                <name>oozie.launcher.mapred.job.queue.name</name>
                <value>${oozieLauncherQueueName}</value>
            </property>
            <property>
                <name>oozie.action.sharelib.for.spark</name>
                <value>${oozieActionShareLibForSpark2}</value>
            </property>
        </configuration>
    </global>

    <start to="document_classification_job" />

	<action name="document_classification_job"> 
        <spark xmlns="uri:oozie:spark-action:0.2">
            
            <master>yarn-cluster</master>
            
            <mode>cluster</mode>
            
            <name>document_classification_job</name>
            
            <class>eu.dnetlib.iis.wf.documentsclassification.DocumentClassificationJob</class>
            <jar>${oozieTopWfApplicationPath}/lib/iis-wf-documentsclassification-${projectVersion}.jar</jar>
            
            <spark-opts>
                --executor-memory=${sparkExecutorMemory}
                --executor-cores=${sparkExecutorCores}
                --driver-memory=${sparkDriverMemory}
                --conf spark.extraListeners=${spark2ExtraListeners}
                --conf spark.sql.queryExecutionListeners=${spark2SqlQueryExecutionListeners}
                --conf spark.yarn.historyServer.address=${spark2YarnHistoryServerAddress}
                --conf spark.eventLog.dir=${nameNode}${spark2EventLogDir}
            </spark-opts>
            
            <arg>-inputAvroPath=${input_documents}</arg>
            <arg>-outputAvroPath=${output_document_to_document_classes}</arg>
            <arg>-scriptDirPath=${wf:conf('oozie.wf.application.path')}/lib/scripts</arg>
            <arg>-numberOfPartitions=${numberOfPartitions}</arg>
            
            <arg>-outputReportPath=${output_report_root_path}/${output_report_relative_path}</arg>
            
        </spark>
        <ok to="end"/>
        <error to="fail"/>
    </action>

    <kill name="fail">
        <message>Unfortunately, the process failed -- error message:
            [${wf:errorMessage(wf:lastErrorNode())}]
        </message>
    </kill>

    <end name="end"/>

</workflow-app>
