{"id": "id0", "text": "Available online at www.sciencedirect.com\nScienceDirect\nProcedia Computer Science 38 (2014) 92 - 99\n10th Italian Research Conference on Digital Libraries, IRCDL 2014\nInformation inference in scholarly communication infrastructures:\nthe OpenAIREplus project experience\nMateusz Kobosa*, Łukasz Bolikowskia, Marek Horsta, Paolo Manghib, Natalia Manolac,\nJochen Schirrwagend\naInterdisciplinary Centre for Mathematical and Computational Modelling, University of Warsaw, Pawinskiego 5a, 02-106 Warsaw, Poland\nbIstituto di Scienza e Tecnologie dell\u0027Informazione “A. Faedo”, Consiglio Nazionale delle Ricerche, via G. Moruzzi 1, 56124 Pisa, Italy\ncDepartment of Informatics and Telecommunications, University of Athens, Panepistimiopolis, 15784 Ilissia, Athens, Greece\ndDepartment of Library Technology and Knowledge Management, Bielefeld University, Universitätsstr. 25, 33615 Bielefeld, Germany\nAbstract\nThe Information Inference Framework presented in this paper provides a general-purpose suite of tools enabling the definition\nand execution of flexible and reliable data processing workflows whose nodes offer application-specific processing capabilities.\nThe IIF is designed for the purpose of processing big data, and it is implemented on top of Apache Hadoop-related technologies\nto cope with scalability and high-performance execution requirements. As a proof of concept we will describe how the\nframework is used to support linking and contextualization services in the context of the OpenAIRE infrastructure for scholarly\ncommunication.\n© 2001144 TThheeAAuutthhoorrss..PPuubblilsishheeddbbyyEElslesveiveirerBB.V. V.. This is an open access article under the CC BY-NC-ND license\nP(hetetrp-:r/e/cvriewatiuvnedcoermrmesopnosn.soirbgi/llitiyceonfstehse/bSyc-inecn-tnifdic/3C.0o/m). mittee of IRCDL 2014.\nPeer-review under responsibility of the Scientific Committee of IRCDL 2014\nKeywords: OpenAIRE infrastructure; data processing system; data mining; text mining; big data\n1. Introduction\nOpenAIREplus1,2 delivers a scholarly communication infrastructure† 3 devised to populate a graph-shaped\ninformation space of interlinked metadata objects describing publications, datasets, persons, projects, and\n* Corresponding author. Tel.: +48 22- 87-49-419.\nE-mail address: mkobos@icm.edu.pl\n† http://ww.openaire.eu\n1877-0509 © 2014 The Authors. Published by Elsevier B.V. This is an open access article under the CC BY-NC-ND license\n(http://creativecommons.org/licenses/by-nc-nd/3.0/).\nPeer-review under responsibility of the Scientific Committee of IRCDL 2014\ndoi:10.1016/j.procs.2014.10.016\nMateusz Kobos et al. / Procedia Computer Science 38 (2014) 92 - 99\n93\norganizations by collecting and aggregating such objects from publication repositories, dataset repositories, and\nCurrent Research Information Systems. The infrastructure supports scientists by providing tools for dissemination\nand discovery of research results, as well as funding bodies and organizations, by providing tools for measuring and\nrefining funding investments in terms of their research impact. In particular, the infrastructure offers tools to enrich\nmetadata objects using inference mechanisms. Such mechanisms, by mining the graph of metadata and original files\nthey describe (e.g. the publication), are capable of identifying new relationships between existing objects, e.g. by\nenriching them with research funding information, supporting data curation, e.g. fixing the title name, or identifying\nnew objects, e.g. by finding missing authors of a publication object.\nIn this paper we present the Information Inference Framework (IIF) which was originally designed to underpin\nthe OpenAIREplus infrastructure inference mechanisms and evolved into a general-purpose solution for processing\nlarge amounts of data in a flexible and scalable way. The IIF can be regarded as a platform for defining and\nexecuting data processing workflows of possibly distributed modules. To this aim, it provides generic tools for: (1)\ndefining domain-specific workflow nodes, e.g. in case of OpenAIRE project: node for extracting metadata from\nPDF files, node for classifying documents; (2) providing well-defined way to pass the data between the nodes; (3)\ndefining sequence of execution of the nodes as a concise workflow; and (4) executing the workflow in a distributed,\nreliable, and scalable computational environment. In order to obtain the benefits described in the last point, we use\nthe Apache Hadoop‡ system that allows for running applications on a computing cluster. IIF is an open source\nproject and its source code is available in the project SVN repository§.\nIn this paper we outline the design decisions and describe the components of this processing infrastructure\nframework for big data. After comparing our work in the context of other popular systems for processing scholarly\ndata in the next section, we show how the framework is applied to enrich scholarly information in the\nOpenAIREplus project.\n2. Related work\nThe IIF is a novel solution. Its contribution can be viewed from two perspectives: introducing a new generic\nframework for processing big data (see Sect. 3), and introducing a new system for processing and extracting\nknowledge from objects related to scientific activity: documents, author information etc. (see Sect. 4).\n2.1. Data processing frameworks\nThe concepts underlying IIF were inspired by the Rapid Miner4 open source software for data analytics. The tool\nallows for creating data workflows consisting of various workflow nodes using a user-friendly graphical interface.\nIts library contains many predefined workflow nodes related to Extract, Transform and Load (ETL) tasks and\nmachine learning algorithms. An interesting feature of this tool is that it checks whether the data declared to be\nconsumed by a workflow node matches the data declared to be produced by a preceding workflow node, which is\ndone as early as during design time of the workflow. A disadvantage of Rapid Miner is that it was not designed to\nprocess large data sets. Our system, on the other hand, was designed from the beginning with scalability and big data\nin mind. Note that there is a plugin for Rapid Miner, called Radoop5 that allows for processing big data by Rapid\nMiner, but this is neither an open source nor free solution and thus not as flexible and customizable as we would\nwant it to be when applying it to our specific domain.\nWith relation to the first perspective, we can also compare the introduced solution with the ones that were\ndevised to solve similar problems. Google Scholar**, Microsoft Academic Search††, ArnetMiner‡‡, and CiteSeerX\n‡ http://hadoop.apache.org/\n§ The project is divided into Maven subprojects with names having “-iis-” infix that are available at https://svn-public.driver.researchinfrastructures.eu/driver/dnet40/modules\n** http://scholar.google.com\n†† http://academic.research.microsoft.com\n‡‡ http://arnetminer.org\n94\nMateusz Kobos et al. / Procedia Computer Science 38 (2014) 92 - 99\nwhich is based on SeerSuite§§ are all popular projects dealing with harvesting and providing access to scientific\npublications and related data. The data processing architecture of ArnetMiner is described in references6,7, and\ninformation related to CiteSeerX can be found in references8,9; unfortunately, we were not able to find any reliable\npublication describing the architecture of neither Microsoft Academic Search nor Google Scholar, so we cannot\ncompare our solution with them. Generally, these projects follow a Service Oriented Architecture (SOA) approach\nwhere each processing module is a separate web service with its own data processing tools. The modules\ncommunicate with each other through well-defined networking channels. Our approach is different: each processing\nmodule is akin to a cluster application that uses the same Hadoop-based computational framework to execute its\ntasks and communicate with other modules. The first approach assures that the modules are very loosely coupled;\nhowever, the disadvantage is that each module has to define its own communication protocol and solution of\nprocessing and storing the data in an efficient way and to handle hardware failures. In our approach, the modules\nprocess the data in the way supported by the Hadoop cluster (note that this does not preclude using a separate web\nservice as a processing module, though it is not as effective as using a native, Hadoop-based approach). This way we\ncan circumvent the mentioned problems with the SOA architecture. See Sect. 3 for a more detailed discussion of\nvarious solutions applied in the IIF along with comparison with other projects.\n2.2. Inferring knowledge over scholarly communication content\nA number of frameworks with objectives and/or architectures similar to IIF can be mentioned. Arguably the two\nmost prominent, complementary architectures for content analysis are Apache UIMA10 and GATE11, which are\nmature, open-source suites of tools for text engineering. Several projects and higher-order frameworks are using\nUIMA or GATE, for example: Behemoth***; or XAR12, an open-source framework for information extraction. The\nIIF addresses the need of a large-scale, flexible, open-source architecture to execute complex workflows for\nprocessing documents (e.g. scientific publications) and finding links between extracted entities; none of the existing\nsolutions can fully satisfy these requirements of the OpenAIREplus project.\nAnother set of solutions related to the second perspective is formed by popular projects related to scientific\npublications mentioned previously (Google Scholar, Microsoft Academic Search, ArnetMiner, and CiteSeerX).\nWhen comparing data extraction and inference functionality of these systems with the IIF as used in OpenAIREplus,\nwe can see that some types of features are shared - although the details differ - between them (extracting metadata\nfrom PDF documents, creating citation links between documents, finding similar documents and clustering\ndocuments, labeling the documents, generating statistics), some features are not present in the IIF, e.g. finding\nexpert in given field (ArnetMiner), finding topics relevant to the query (ArnetMiner), organization comparison\n(Microsoft Academic Search), showing co-authorship graph (Microsoft Academic Search Engine), but some\nfeatures are present in the IIF only: extracting links to projects and related funding source from a document, finding\nsimilarities between documents based on activity of users of the OpenAIRE portal, extracting links to related data\nsets from a document. The last one allows OpenAIREplus users to easily access research data described in\npublications and is of paramount importance since the scientific research and scholarly communication is becoming\nincreasingly data-centric.\n3. The Information Inference Framework\nThe IIF is essentially a flexible data processing system for big data. It was designed to fulfil the requirements of\nthe OpenAIREplus project, but built to be generic and not restricted to this specific application domain. The IIF\nprovides tools for defining data processing workflows which consist of: (1) workflow nodes responsible for data\nprocessing, (2) data stores that serve as means of communication between workflow nodes, and (3) tools for\nexecuting a workflow.\n§§ http://citeseerx.ist.psu.edu, http://citeseerx.sourceforge.net\n*** https://github.com/DigitalPebble/behemoth\nMateusz Kobos et al. / Procedia Computer Science 38 (2014) 92 - 99\n95\nAs mentioned above, the framework was implemented on top of Apache Hadoop in order to benefit from its outof-the-box\nfacilities to scale in terms of size and in terms of distributed computation. In Apache Hadoop, data files\nare stored in its distributed file system (HDFS) which is designed to flexibly scale by simply integrating new nodes\nto the underlying cluster. Moreover, the business logic of workflow nodes is written using the MapReduce\nprogramming paradigm and their execution is automatically distributed over the cluster nodes. Last but not least,\nhardware failures of the machines belonging to the cluster are handled automatically.\nIn the following part, we will describe workflow nodes, data stores, and workflows along with related design and\ntechnological decisions and compare them with alternative solutions applied in other systems.\n3.1. Data stores\nThe data objects passed between workflow nodes are named “data stores”. Each data store corresponds to a list of\nobjects, where each object is of the same type, i.e. it is defined by the same schema. We considered a few alternative\nserialization formats for data stores: plain CSV files, JSON, Thrift, Protocol Buffers, and Avro. Among these we\nhave chosen the Avro datafile††† since it is characterized by a combination of useful features which are unique\namong these technologies: it defines a schema being kept with the data it describes, it uses an efficient binary\nstorage format, it is forward/backward compatible, and it is well integrated with the Hadoop ecosystem (e.g. it is\nhandled out-of-the-box by MapReduce jobs and Hadoop Pig scripts).\nData stores are immutable, i.e. they cannot be modified after they are created. This approach has two main\nadvantages. First, since the data is read-only, it removes the need for synchronization. This is very important since\nproblems with synchronization of access to a shared file system (e.g. in CiteSeerX) result in frequent freezes of the\nsystem (see8). Second, it makes reasoning about the state of the system as well as debugging and restoring of the\nworkflow state easier, thereby facilitating workflow nodes development. This solution is akin to the approach of\n“sharing memory by communicating” applied in the concurrency-centered system language Go‡‡‡. The side-effect of\nimmutability is data replication, which can be tackled by a garbage collector set to remove useless data stores.\nData stores are kept in the Hadoop distributed file system (HDFS). Each data store is represented by a single\nHDFS directory which contains a set of Avro datafiles with the same schema. Before chosing HDFS, we considered\nand compared several types of databases. For example ArnetMiner stores its data in a RDF data base backed by a\nrelational MySQL database, while CiteSeerX stores the data in a MySQL relational database and in a distributed\nGlobal File System. A disadvantage of both approaches is that they rely on RDMSs which are not designed to\narbitrarily scale unless expensive manually distributed solutions (e.g. horizontal/vertical partitioning) are adopted.\nWe also considered using freely available RDF databases, but they seemed to be not scalable enough for our\nneeds§§§.\n3.2. Workflow nodes\nA workflow node is defined by: (1) the schemas of input and output data stores, and (2) its business logic (see\nFig. 1 for an example), which processes input data stores and generates output data stores conforming to the given\nschemas. Note that this solution is closely related to the “pipes and filters” design pattern13. The schemas, in a way\nsimilar to static typing in programming languages, define input and output interfaces of the workflow node. This\napproach reduces somewhat flexibility but increases modularity and enables static workflow correctness control; i.e.\nchecking, before the execution of the workflow, whether the data store consumed by a downstream workflow node\nis compatible with the data store produced by an upstream workflow node. This modularity results in loose coupling\nof workflow nodes, an important feature of every system; for comparison, in CiteSeerX and ArnetMiner this is\nachieved by applying a Service Oriented Architecture approach where each module is a separate web service.\n††† http://avro.apache.org\n‡‡‡ http://blog.golang.org/share-memory-by-communicating\n§§§ http://www.w3.org/wiki/LargeTripleStores website contains list of sizes of such databases in real-life deployments\n96\nMateusz Kobos et al. / Procedia Computer Science 38 (2014) 92 - 99\nTo specify the input and output interfaces, we use Avro schemas. This approach results in an interesting\nfunctionality akin to polymorphic behavior of arguments of methods in object-oriented languages: a workflow node\ncan handle not only the data stores that have exactly the same schema as defined by its input, but also data stores\nwith a schema that is a superset of the expected schema. This way the workflow node can define that it needs only a\npart of a possibly complex data store and only this part is visible to the node.\nTypes of workflow nodes. The system supports the definition of various kinds of workflow nodes; they can be\ngenerally divided into two categories: atomic and composite workflow nodes.\nAn atomic node is indivisible from the point of view of the IIF. It might be a map-reduce Hadoop job or a chain\nof such jobs that uses the underlying Hadoop system; it can also be a wrapper for some external system, e.g. a web\nservice that processes the data using its own tools like a neo4j database.\nA composite node consists of a combination of workflow nodes. Currently we define the following composite\nworkflow nodes: subworkflow, conditional execution workflow node (one of the control paths is chosen to be\nexecuted based on whether a certain condition is satisfied), parallel execution workflow node (several control paths\nare executed in parallel).\nNote that it is often the case that a data store produced by one workflow node does not match exactly the data\nstore expected by a node that is supposed to consume it, e.g. a certain attribute in one data store is represented by\ntwo attributes in the other data store. Thus, the data store produced by one workflow node has to be somehow\nadapted before it can be consumed by the subsequent node. A workflow node that is responsible for such\ntransformation is named “transformer” - note that its role is similar to the “message translator” pattern13.\nTransformers are usually written as Pig scripts which provide a succinct data transformation language similar to\nSQL. When executed on a cluster, the scripts are automatically translated into a series of MapReduce jobs.\nFig. 1. A sample OpenAIREplus workflow node that ingests data stores corresponding to projects and full text of documents and produces a data\nstore containing links between documents and projects. The node specifies its inputs and outputs with Avro schemas, which are represented here\nas UML class diagrams.\n3.3. Definition and execution of a data processing workflow\nAfter concrete workflow nodes appropriate for a given application domain are implemented, the workflow\ndesigner creates a workflow definition which connects outputs of certain workflow nodes with inputs of other\nworkflow nodes, thus defining a data processing pipeline. Finally, the framework creates an archive containing the\nworkflow definition file and implementations of workflow nodes (which are stored in *.jar files). Next, such\narchives are automatically deployed and executed on the cluster.\nWhen choosing the workflow engine specific to our use case, a couple of alternatives have been considered:\ngeneric ones (Apache ODE, Sarasvati) and based on Hadoop (Apache Oozie, LinkedIn\u0027s Azkaban, Hamake). We\ndecided to choose probably the most popular engine from the second group: Apache Oozie. We were convinced by\nMateusz Kobos et al. / Procedia Computer Science 38 (2014) 92 - 99\n97\nits balanced mix of properties: relative maturity of the project; queuing and coordination of the workflow done\ndirectly on the Hadoop cluster (through Oozie service); possibility to restart the workflow from a certain point; and\nvariety of supported interfaces (command-line, REST, Java API, web interface for browsing executed workflows).\nNote that Apache Oozie is a generic workflow engine, without any notion of the data passed between workflow\nnodes. To implement our idea of a data processing workflow, we impose certain conventions on the way of defining\nour Oozie workflows: each workflow node (1) must clearly specify what kind of data it consumes/produces (2) has a\nplace to store its intermediary computations results, (3) stores output data in well-defined data stores, and (4)\nreceives input parameters in a well-defined manner. In order to hide such conventions from the developers, we are\ncurrently in the process of designing a data workflow description language on top of Oozie to write workflows\naccording to such conventions; this approach results in a more concise, uniform, legible, and easy to maintain\nworkflow descriptions.\nFig. 2. Architecture of OpenAIREplus system. Arrows show the direction of data flow along with information about type of data being\ntransferred.\n4. The IIF in OpenAIREplus\nThe original motivation behind developing the IIF was implementing information inference service for scholarly\ncommunication infrastructure of the OpenAIREplus project. In this section we present the OpenAIREplus project\nitself and the role of the IIF in it.\n4.1. Architecture of the OpenAIREplus system\nA high-level architecture of the OpenAIREplus infrastructure is presented in Fig. 2. Its core components are:\n• Data aggregation and persistence. The data aggregation framework is based on the D-NET Software Toolkit14,\nwhich offers tools for defining and executing metadata processing workflows. In OpenAIREplus, the framework\nhas been configured to collect XML-serialized versions of publications, datasets, and projects from data sources\nof various types, such as publication repositories, dataset repositories, CRIS systems, entity registries (e.g.\nOpenDOAR, re3data.org). Subsequently, such serializations are harmonized and transformed to become objects\nof a graph of entities (i.e. publications, datasets, funding schemes, projects, organisations, persons, and data\nsources). The graph is stored in the Information Space which can be regarded as the central database of the\nsystem.\n• Data access. The data stored in the Information Space can be accessed using the OpenAIRE portal and standard\nmachine communication protocols (e.g. OAI-PMH, CQL).\n• Information inference. An instance of the IIF is responsible for enriching the data stored in the Information\nSpace with new information using various types of data mining algorithms.\n98\nMateusz Kobos et al. / Procedia Computer Science 38 (2014) 92 - 99\nThe IIF includes several workflow nodes and integrates them in one data inference workflow. The subset of data\nfrom the Information Space that is required by the workflow nodes is retrieved by the IIF importer module. Next,\nthe IIF executes the workflow and produces an output which is subsequently merged with the original data in the\nInformation Space, in order to enrich or adjust its content. The data stored and produced by the IIF forms a\nsecondary, non-authoritative storage, i.e. at any moment, all data in the IIF can be recreated from the Information\nSpace by re-running the IIF workflow. IIF is stateless in this sense.\nwebsite users\nactions\nbase\n1. Import objects\n2. Website usage\n3. Document content\nanalysis\nwebsite\nsimilarities\nextracted\ndocument data\n4. Documents\nclassification\n5. Documents\nclustering\n6. Metrics\nmetrics\nclasifications\nclusters\n7. Documents\nsimilarity\n8. Export\nsimilarities\nFig. 3. General workflow of the IIF. Thick arrows correspond to outputs of workflow nodes, thin arrows correspond to connections between\ninputs and outputs.\n4.2. Information Inference Framework in OpenAIREplus\nThe instance of the IIF deployed in OpenAIREplus defines specific workflow nodes and related data stores. In\ngeneral, it processes plain text of papers retrieved from the Information Space and their related metadata and\nproduces various inferred relationships and data. See Fig. 1 for a sample definition of an OpenAIREplus workflow\nnode. The functionalities provided by the IIF instance to OpenAIREplus system correspond directly to the following\ntop-level workflow nodes that are bound together by OpenAIREplus inference workflow (see Fig. 3):\n1. Import - retrieves relevant data from the Information Space and saves it in data stores. The data stores contain\ninformation about persons, projects, documents, datasets, and logs from the OpenAIREplus web portal.\n2. Website usage - computes similarities between documents and persons based on the activity of the\nOpenAIREplus web portal users, e.g. if a user views two documents in a short time, they might be related.\n3. Document content analysis - extracts metadata from PDF documents and uses it to update the original\ninformation about documents. This node consists of a few smaller subworkflows that provide the following\nfunctionalities: extracting basic metadata from raw content of documents (PDF files); creating “cited by”\nrelationships between documents based on the references in document\u0027s bibliography section; creating\n“funded by” relationships between document and projects retrieved from the Information Space; creating\n“references” relationships between a document and its data sets stored in selected external databases (e.g.\nDataCite, EuropePMC).\n4. Documents classification - assigns predefined labels (e.g. based on ArXiv taxonomy) to documents based on\ntheir contents, metadata, and activity of OpenAIREplus web portal users.\n5. Documents clustering - creates clusters of similar documents. The similarity is computed based on document\ncontent and its metadata as well as the activity of OpenAIREplus web portal users.\n6. Metrics - computes basic metrics for authors and documents, such as number of documents produced by a\ngiven author and number of citations of a given document.\n7. Documents similarity - computes which documents are similar and thus are possibly related or duplicated\nbased on the data gathered so far.\n8. Export - exports inferred data to the Information Space.\nMateusz Kobos et al. / Procedia Computer Science 38 (2014) 92 - 99\n99\n4.3. Current state of development and future plans\nThe IIF is still actively developed. However, the most important framework elements are already functional and we\ncurrently integrate the nodes provided by OpenAIREplus project partners into one workflow. These include\nCERMINE**** system for metadata extraction and MadIS†††† system for extraction of references from documents\nand text mining. In the next months, other nodes providing the functionality described above will be incrementally\nintroduced to complete the system by the end of 2015. In the same time-frame, we are considering to introduce an\nadditional “training” workflow. Its goal would be to automatically adjust machine learning models (responsible, e.g.\nfor classification and clustering functionalities) to changes in Information Space data. Currently these models have\nto be provided by the developers.\nAcknowledgments\nReferences\nThis work is funded by the European commission (FP7-INFRA-2011-2, Grant Agreement no. 283595). This\nwork would have not been possible without the contributions and experiences of the OpenAIREplus technical team\nmembers: M. Artini, C. Atzori, A. Bardi, S. La Bruzzo, M. Mikulicic (ISTI-CNR, Italy), L. Nielsen (CERN,\nSwitzerland), B. Companjen (DANS-KNAW, Netherlands), T. Giannakopoulos, H. Dimitropoulos, K. Iatropoulou,\nA. Lempesis, O. Metaxas (University of Athens, Greece), M. Loesch (Bielefeld University, Germany).\n1. Manghi P, Bolikowski L, Manola N, Schirrwagen J, Smith T. OpenAIREplus: the European Scholarly Communication Data Infrastructure. DLib\nMagazine 2012; 18.\n2. Manghi P, Manola N, Horstmann W, Peters D. An Infrastructure for Managing EC Funded Research Output - The OpenAIRE Project.\nInternational Journal on Grey Literature 2010; 6: 31-40.\n3. Castelli D, Manghi P, Thanos C. A vision towards scientific communication infrastructures. International Journal on Digital Libraries 2013;\n13: 55-169.\n4. Mierswa I, Wurst M, Klinkenberg R, Scholz M, Euler T. Yale. Rapid prototyping for complex data mining tasks. In: Ungar L, Craven M,\nGunopulos D, Eliassi-Rad T, editors. KDD \u002706: Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery\nand data mining, New York: ACM; 2006, p. 935-940.\n5. Prekopcsák Z, Makrai G, Henk T, Gáspár-Papanek C. Radoop. Analyzing big data with rapidminer and hadoop. In: Fisher S, Mierswa I,\neditors. Proceedings of the 2nd RapidMiner Community Meeting and Conference, Aachen: Shaker Verlag; 2011, p. 31-42.\n6. Tang J, Zhang J, Zhang D, Yao L, Zhu C, Li JZ. ArnetMiner: An Expertise Oriented Search System for Web Community. In: Golbeck J, Mika\nP, editors. Proceedings of the Semantic Web Challenge 2007, CEUR-WS.org; 2007.\n7. Tang J, Zhang J, Yao L, Li J, Zhang L, Su Z. ArnetMiner. extraction and mining of academic social networks. In: Proceedings of the 14th\nACM SIGKDD international conference on knowledge discovery and data mining. KDD \u002708, New York: ACM; 2007, p. 990-998.\n8. Teregowda PB, Councill IG, Fernández RJP, Khabsa M, Zheng S, Giles CL. SeerSuite: developing a scalable and reliable application\nframework for building digital libraries by crawling the web. In: Proceedings of the 2010 USENIX conference on Web application\ndevelopment. WebApps\u002710, Berkeley: USENIX Association; 2010, p. 14-14.\n9. Li H, Councill I, Lee WC, Giles CL. CiteSeerx: an architecture and web service design for an academic document search engine. In:\nProceedings of the 15th international conference on World Wide Web, New York: ACM; 2006, p. 883-884.\n10. Ferrucci D, Lally A. UIMA: an architectural approach to unstructured information processing in the corporate research environment. Nat\nLang Eng 2004; 10: 327-348\n11. Cunningham H, Maynard D, Bontcheva K, Tablan V, Aswani N, Roberts I, Gorrell G, Funk A, Roberts A, Damljanovic D, Heitz T,\nGreenwood MA, Saggion H, Petrak J, Li Y, Peters W. Text Processing with GATE (Version 6). California: Gateway Press; 2011.\n12. Ashish N, Mehrotra S, Pirzadeh P. XAR: An Integrated Framework for Information Extraction. In: Computer Science and Information\nEngineering, 2009 WRI World Congress on. Los Angeles: IEEE; 2009, p. 462-466.\n13. Hohpe G, Woolf B. Enterprise Integration Patterns: designing, building, and deploying messaging solutions. Addison-Wesley 2004.\n14. Manghi P, Artini M, Atzori C, Bardi A, Mannocci A, La Bruzzo S, Candela L, Castelli D, Pagano P. The D-NET software toolkit: A\nframework for the realization, maintenance, and operation of aggregative infrastructures. Journal Program, Emerald Insight 2014;48(4):322354.\n**** https://github.com/CeON/CERMINE\n†††† http://code.google.com/p/madis"}
{"id": "id1", "text": "This is sample PDF document.\nGood man, I\u0027m a good man, food\u0027s on the table, working two jobs,\nWaiting willing the neighbors check your name\nI\u0027m having fun, got no kids and I love the Lord, check your name\nI\u0027m monogamous never did time but maybe just once, check your name\nAnd I puts it down, wanna see it twice, brother puts it down, check\nHow could you ever walk away, after all I\u0027ve done for you\nI feel like there\u0027s a knife in my back, babe\nYou might as well pull it all the way through\nGood man, I\u0027m a good man, food\u0027s on the table, working two jobs,\nWaiting willing the neighbors check your name, good man\nI\u0027m having fun, got no kids and I love the Lord, check your name\nI\u0027m monogamous never did time but maybe just once, check your name\nAnd I puts it down, wanna see it twice, brother puts it down, check\nYou watched them hand cuff me lady, and walk me to my room\nI never been so humiliated, I don\u0027t think that I deserve it for you\nGood man, I\u0027m a good man, food\u0027s on the table, working two jobs,\nWaiting willing the neighbors check your name, good man\nI\u0027m having fun, got no kids and I love the Lord, check your name\nI\u0027m a monogamus never did time but maybe just once, check your name\nAnd I puts it down, wanna see it twice, brother puts it down, check\nI never been, I never been so much pain, never\nMy life is better, my love is pretty, so much better, now\nWithout you, I never had so ... without you, whenever ... inside\nHow could you... just ...walk away\nWhen the things go rough you quickly ran into .. somebody say, I\u0027ve been there\nThere\u0027s one thing I know so I\u0027ma tell you that I\u0027m a good man, back to you, a good man\nGood man, good man, good man, good man."}