<?xml version="1.0"?>
<workflow-app xmlns="uri:oozie:workflow:0.5" name="spark_pipe_cloner (core-examples)">
	<start to="producer" />
	
	<action name="producer">
		<java>
			<job-tracker>${jobTracker}</job-tracker>
			<name-node>${nameNode}</name-node>
			<!-- The data generated by this node is deleted in this section -->
			<prepare>
				<delete path="${nameNode}${workingDir}/producer" />
				<mkdir path="${nameNode}${workingDir}/producer" />
			</prepare>
			<configuration>
				<property>
					<name>mapred.job.queue.name</name>
					<value>${queueName}</value>
				</property>
			</configuration>
			<!-- This is simple wrapper for the Java code -->
			<main-class>eu.dnetlib.iis.core.java.ProcessWrapper</main-class>
			<!-- The business Java code that gets to be executed -->
			<arg>eu.dnetlib.iis.core.examples.java.SampleDataProducer</arg>
			<!-- All input and output ports have to be bound to paths in HDFS -->
			<arg>-Operson=${workingDir}/producer/person</arg>
			<arg>-Odocument=${workingDir}/producer/document</arg>
		</java>
		<ok to="spark_pipe_cloner" />
		<error to="fail" />
	</action>


	<action name="spark_pipe_cloner">

		<spark xmlns="uri:oozie:spark-action:0.1">

			<job-tracker>${jobTracker}</job-tracker>
			<name-node>${nameNode}</name-node>

			<prepare>
				<delete path="${nameNode}${workingDir}/cloner" />
				<mkdir path="${nameNode}${workingDir}/cloner" />
			</prepare>

			<master>yarn-cluster</master>

			<mode>cluster</mode>

			<name>spark_pipe_cloner</name>

			<class>eu.dnetlib.iis.core.spark.pipe.SparkPipeMapReduce</class>

			<jar>${wf:conf('oozie.wf.application.path')}/lib/iis-workflows-core-examples-${projectVersion}.jar
			</jar>

			
			<arg>-inputAvroPath = ${workingDir}/producer/person</arg>
			<arg>-inputAvroSchemaClass =
				eu.dnetlib.iis.core.examples.schemas.documentandauthor.Person</arg>

			<arg>-outputAvroPath = ${workingDir}/cloner/person</arg>
			<arg>-outputAvroSchemaClass =
				eu.dnetlib.iis.core.examples.schemas.documentandauthor.Person</arg>

			<arg>-mapperScript = ${wf:conf('oozie.wf.application.path')}/scripts/cloner.py</arg>
			<arg>-mapperScriptArgs = --copies 3</arg>

			<arg>-reducerScript = ${wf:conf('oozie.wf.application.path')}/scripts/cloner.py</arg>
			<arg>-reducerScriptArgs = --copies 2</arg>

		</spark>

		<ok to="consumer" />

		<error to="fail" />

	</action>

	<action name="consumer">
		<java>
			<job-tracker>${jobTracker}</job-tracker>
			<name-node>${nameNode}</name-node>
			<configuration>
				<property>
					<name>mapred.job.queue.name</name>
					<value>${queueName}</value>
				</property>
			</configuration>
			<!-- This is simple wrapper for the Java code -->
			<main-class>eu.dnetlib.iis.core.java.ProcessWrapper</main-class>
			<!-- The business Java code that gets to be executed -->
			<arg>eu.dnetlib.iis.core.examples.java.PersonTestingConsumer</arg>
			<!-- All input and output ports have to be bound to paths in HDFS -->
			<arg>-Iperson=${workingDir}/cloner/person</arg>
			<arg>-Pexpected_copies=6</arg>
		</java>
		<ok to="end" />
		<error to="fail" />
	</action>
	<kill name="fail">
		<message>Unfortunately, the process failed -- error message:
			[${wf:errorMessage(wf:lastErrorNode())}]
		</message>
	</kill>
	<end name="end" />
</workflow-app>


